{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "\tdef __init__(self, w_num):\n",
    "\t\tself.input = []\n",
    "\t\tself.output = 0\n",
    "\t\tself.weights = []\n",
    "\t\tself.bias = 0\n",
    "\t\tself.delta = 0\n",
    "\t\tself.bias = np.random.randn()\n",
    "\t\tfor i in range(w_num):\n",
    "\t\t\tself.weights.append(np.random.randn())\n",
    "\t\n",
    "\tdef forward(self, inp):\n",
    "\t\tif len(inp) != len(self.weights):\n",
    "\t\t\traise Exception(\"input and weight's length mismatch!\")\n",
    "\t\tself.input = inp\n",
    "\t\tself.output = 0\n",
    "\t\tfor i in range(len(self.input)):\n",
    "\t\t\tself.output += self.weights[i] * self.input[i]\n",
    "\t\tself.output = self.sigmoid(self.output + self.bias)\n",
    "\t\treturn self.output\n",
    "\t\n",
    "\tdef sigmoid(self, x):\n",
    "\t\treturn 1/(1+np.exp(-x))\n",
    "\t\n",
    "\tdef derivative_sigmoid(self, x):\n",
    "\t\treturn np.multiply(x, 1.0 - x)\n",
    "\t\n",
    "\tdef cal_delta(self, error):\n",
    "\t\tself.delta = error * self.derivative_sigmoid(self.output)\n",
    "\n",
    "\tdef update(self, learnRate):\n",
    "\t\tfor i in range(len(self.weights)):\n",
    "\t\t\tself.weights[i] -= learnRate * self.delta * self.input[i]\n",
    "\t\tself.bias -= learnRate * self.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer(object):\n",
    "\tdef __init__(self, input_ch, neuron_num):\n",
    "\t\tself.neurons = []\n",
    "\t\tfor i in range(neuron_num):\n",
    "\t\t\tneuron = Neuron(input_ch)\n",
    "\t\t\tself.neurons.append(neuron)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\toutput = []\n",
    "\t\tfor i in range(len(self.neurons)):\n",
    "\t\t\toutput.append(self.neurons[i].forward(inp))\n",
    "\t\treturn output\n",
    "\n",
    "\tdef get_deltas(self, pre_layer):\n",
    "\t\tdeltas = []\n",
    "\t\tpre_l_neurons = pre_layer.neurons\n",
    "\t\tfor i in range(len(self.neurons)):\n",
    "\t\t\terror = 0\n",
    "\t\t\tfor pre_l_neuron in pre_l_neurons:\n",
    "\t\t\t\terror += pre_l_neuron.delta * pre_l_neuron.weights[i]\n",
    "\t\t\tself.neurons[i].cal_delta(error)\n",
    "\t\t\tdeltas.append(self.neurons[i].delta)\n",
    "\t\treturn deltas\n",
    "\n",
    "\tdef update(self, learnRate):\n",
    "\t\tfor neuron in self.neurons:\n",
    "\t\t\tneuron.update(learnRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\tdef __init__(self, learnRate, debug=False):\n",
    "\t\tself.layers = []\n",
    "\t\tself.learnRate = learnRate\n",
    "\t\tself.debug = debug\n",
    "\n",
    "\tdef train(self, dataset):\n",
    "\t\tinputs, labels = dataset\n",
    "\t\tfor i in range(len(inputs)):\n",
    "\t\t\tself.forward(inputs[i])\n",
    "\t\t\tself.backpropagate(labels[i])\n",
    "\t\t\tself.update()\n",
    "\t\treturn labels[i] - self.layers[-1].neurons[0].output\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tx = inp\n",
    "\t\tfor i in range(len(self.layers)):\n",
    "\t\t\tx = self.layers[i].forward(x)\n",
    "\t\t\tif self.debug:\n",
    "\t\t\t\tprint(\"Layer {0} output {1}\".format(i, x))\n",
    "\t\treturn x\n",
    "\n",
    "\tdef backpropagate(self, label):\n",
    "\t\tlast_layer = None\n",
    "\t\tdeltas = []\n",
    "\t\tfor i in range(len(self.layers), 0, -1):\n",
    "\t\t\tcurrent_layer = self.layers[i-1]\n",
    "\t\t\tif last_layer is None:\n",
    "\t\t\t\tfor i in range(len(current_layer.neurons)):\n",
    "\t\t\t\t\terror = -(label - current_layer.neurons[i].output)\n",
    "\t\t\t\t\tcurrent_layer.neurons[i].cal_delta(error)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdeltas = current_layer.get_deltas(last_layer)\n",
    "\t\t\tlast_layer = current_layer\n",
    "\t\t\tif self.debug:\n",
    "\t\t\t\tprint(\"Layer {0} deltas {1}\".format(i, deltas))\n",
    "\n",
    "\tdef update(self):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tlayer.update(self.learnRate)\n",
    "\n",
    "\tdef predict(self, inp):\n",
    "\t\treturn self.forward(inp)\n",
    "\n",
    "\tdef add_layer(self, layer):\n",
    "\t\tself.layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear(n=100):\n",
    "    pts = np.random.uniform(0, 1, (n, 2))\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for pt in pts:\n",
    "        inputs.append([pt[0], pt[1]])\n",
    "        distance = (pt[0]-pt[1])/1.414\n",
    "        if pt[0] > pt[1]:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return inputs, labels\n",
    "\n",
    "def generate_XOR_easy():\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(11):\n",
    "        inputs.append([0.1*i, 0.1*i])\n",
    "        labels.append(0)\n",
    "\n",
    "        if 0.1*i == 0.5:\n",
    "            continue\n",
    "\n",
    "        inputs.append([0.1*i, 1-0.1*i])\n",
    "        labels.append(1)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(inputs, labels, pred_y):\n",
    "\tplt.subplot(1,2,1)\n",
    "\tplt.title('Ground truth', fontsize=18)\n",
    "\tfor i in range(len(inputs)):\n",
    "\t\tif labels[i] - 0 < 1e-2:\n",
    "\t\t\tplt.plot(inputs[i][0], inputs[i][1], 'ro')\n",
    "\t\telse:\n",
    "\t\t\tplt.plot(inputs[i][0], inputs[i][1], 'bo')\n",
    "\tplt.subplot(1,2,2)\n",
    "\tplt.title('Predict result', fontsize=18)\n",
    "\tfor i in range(len(inputs)):\n",
    "\t\tif pred_y[i] - 0 < 1e-2:\n",
    "\t\t\tplt.plot(inputs[i][0], inputs[i][1], 'ro')\n",
    "\t\telse:\n",
    "\t\t\tplt.plot(inputs[i][0], inputs[i][1], 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnRate = 0.5\n",
    "epoch = 10000\n",
    "\n",
    "dataset = generate_XOR_easy()\n",
    "\n",
    "input_channel = len(dataset[0][0])\n",
    "layers = [input_channel,4,4,1]\n",
    "neuron_layer = []\n",
    "nn = NeuralNetwork(learnRate = learnRate, debug=False)\n",
    "for i in range(len(layers)-1):\n",
    "    weight = []\n",
    "    bias = np.random.randn()\n",
    "    for j in range(layers[i]*layers[i+1]):\n",
    "        weight.append(np.random.randn())\n",
    "    layer = NeuralLayer(input_ch=layers[i], neuron_num=layers[i+1])\n",
    "    nn.add_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.2731821367362006\n",
      "epoch 100 loss 0.5423772221645842\n",
      "epoch 200 loss 0.5495562084217449\n",
      "epoch 300 loss 0.5619634105128222\n",
      "epoch 400 loss 0.08936557213115914\n",
      "epoch 500 loss 0.01769839440121357\n",
      "epoch 600 loss 0.004406613940631132\n",
      "epoch 700 loss 0.0020947829723436673\n",
      "epoch 800 loss 0.0013918251904559975\n",
      "epoch 900 loss 0.0010568169695549523\n",
      "epoch 1000 loss 0.0008604856631644342\n",
      "epoch 1100 loss 0.0007310709711014773\n",
      "epoch 1200 loss 0.0006390545805754488\n",
      "epoch 1300 loss 0.0005700822010035189\n",
      "epoch 1400 loss 0.0005163333517894664\n",
      "epoch 1500 loss 0.0004731809406564036\n",
      "epoch 1600 loss 0.000437710154908566\n",
      "epoch 1700 loss 0.00040799201571850574\n",
      "epoch 1800 loss 0.0003826980999790397\n",
      "epoch 1900 loss 0.0003608830299560717\n",
      "epoch 2000 loss 0.00034185523365637405\n",
      "epoch 2100 loss 0.0003250968060792836\n",
      "epoch 2200 loss 0.00031021198696423724\n",
      "epoch 2300 loss 0.00029689299172741546\n",
      "epoch 2400 loss 0.00028489673159004525\n",
      "epoch 2500 loss 0.00027402857334013486\n",
      "epoch 2600 loss 0.000264130770842419\n",
      "epoch 2700 loss 0.0002550740696830589\n",
      "epoch 2800 loss 0.0002467515121834518\n",
      "epoch 2900 loss 0.0002390737968810397\n",
      "epoch 3000 loss 0.00023196575478712234\n",
      "epoch 3100 loss 0.00022536364030201206\n",
      "epoch 3200 loss 0.00021921302471927806\n",
      "epoch 3300 loss 0.0002134671411723188\n",
      "epoch 3400 loss 0.00020808557176665854\n",
      "epoch 3500 loss 0.0002030331968969623\n",
      "epoch 3600 loss 0.0001982793474718525\n",
      "epoch 3700 loss 0.00019379711562572677\n",
      "epoch 3800 loss 0.00018956279030235557\n",
      "epoch 3900 loss 0.0001855553920098174\n",
      "epoch 4000 loss 0.0001817562869371736\n",
      "epoch 4100 loss 0.00017814886502620642\n",
      "epoch 4200 loss 0.0001747182699224359\n",
      "epoch 4300 loss 0.0001714511712798128\n",
      "epoch 4400 loss 0.00016833557183359993\n",
      "epoch 4500 loss 0.00016536064318839472\n",
      "epoch 4600 loss 0.00016251658542987268\n",
      "epoch 4700 loss 0.0001597945066160733\n",
      "epoch 4800 loss 0.00015718631892225332\n",
      "epoch 4900 loss 0.00015468464880885602\n",
      "epoch 5000 loss 0.00015228275903955701\n",
      "epoch 5100 loss 0.00014997448076337072\n",
      "epoch 5200 loss 0.00014775415416479198\n",
      "epoch 5300 loss 0.00014561657644907022\n",
      "epoch 5400 loss 0.00014355695611645292\n",
      "epoch 5500 loss 0.00014157087265442847\n",
      "epoch 5600 loss 0.00013965424091055834\n",
      "epoch 5700 loss 0.00013780327952161997\n",
      "epoch 5800 loss 0.00013601448286448825\n",
      "epoch 5900 loss 0.00013428459608388899\n",
      "epoch 6000 loss 0.00013261059280000875\n",
      "epoch 6100 loss 0.00013098965517177596\n",
      "epoch 6200 loss 0.00012941915602582288\n",
      "epoch 6300 loss 0.0001278966428038819\n",
      "epoch 6400 loss 0.00012641982311922817\n",
      "epoch 6500 loss 0.00012498655173209805\n",
      "epoch 6600 loss 0.0001235948187845448\n",
      "epoch 6700 loss 0.00012224273915517614\n",
      "epoch 6800 loss 0.00012092854281131604\n",
      "epoch 6900 loss 0.00011965056604668067\n",
      "epoch 7000 loss 0.00011840724351719345\n",
      "epoch 7100 loss 0.00011719710098434533\n",
      "epoch 7200 loss 0.00011601874869826556\n",
      "epoch 7300 loss 0.00011487087535366758\n",
      "epoch 7400 loss 0.00011375224255971705\n",
      "epoch 7500 loss 0.0001126616797773039\n",
      "epoch 7600 loss 0.00011159807967298097\n",
      "epoch 7700 loss 0.00011056039385481942\n",
      "epoch 7800 loss 0.00010954762894965775\n",
      "epoch 7900 loss 0.00010855884299387775\n",
      "epoch 8000 loss 0.00010759314210539994\n",
      "epoch 8100 loss 0.00010664967741336184\n",
      "epoch 8200 loss 0.00010572764222283038\n",
      "epoch 8300 loss 0.00010482626939134487\n",
      "epoch 8400 loss 0.0001039448289017475\n",
      "epoch 8500 loss 0.00010308262561298243\n",
      "epoch 8600 loss 0.00010223899717298757\n",
      "epoch 8700 loss 0.0001014133120813554\n",
      "epoch 8800 loss 0.00010060496788921736\n",
      "epoch 8900 loss 9.981338952469443e-05\n",
      "epoch 9000 loss 9.903802773270076e-05\n",
      "epoch 9100 loss 9.827835762354908e-05\n",
      "epoch 9200 loss 9.753387731592511e-05\n",
      "epoch 9300 loss 9.68041066737868e-05\n",
      "epoch 9400 loss 9.608858612386584e-05\n",
      "epoch 9500 loss 9.538687555021852e-05\n",
      "epoch 9600 loss 9.469855326249554e-05\n",
      "epoch 9700 loss 9.402321502571809e-05\n",
      "epoch 9800 loss 9.3360473155224e-05\n",
      "epoch 9900 loss 9.270995566534879e-05\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "inputs, labels = dataset\n",
    "for i in range(epoch):\n",
    "    loss = nn.train(dataset)\n",
    "    loss_sum = 0\n",
    "    if i % 100 == 0:\n",
    "        loss_sum = np.mean(loss_sum + loss) \n",
    "        print(\"epoch {0} loss {1}\".format(i, loss_sum))\n",
    "    else:\n",
    "        loss_sum += loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [0.0, 0.0] , label 0, predict [0.00012496193308700822]\n",
      "input [0.0, 1.0] , label 1, predict [0.99989869686229083]\n",
      "input [0.1, 0.1] , label 0, predict [0.00029177292420249517]\n",
      "input [0.1, 0.9] , label 1, predict [0.99990140687655815]\n",
      "input [0.2, 0.2] , label 0, predict [0.00099355863782728065]\n",
      "input [0.2, 0.8] , label 1, predict [0.99990038228375699]\n",
      "input [0.30000000000000004, 0.30000000000000004] , label 0, predict [0.0033539531215595404]\n",
      "input [0.30000000000000004, 0.7] , label 1, predict [0.99986232622217952]\n",
      "input [0.4, 0.4] , label 0, predict [0.007199051393484061]\n",
      "input [0.4, 0.6] , label 1, predict [0.98940025109655094]\n",
      "input [0.5, 0.5] , label 0, predict [0.0086436230796838372]\n",
      "input [0.6000000000000001, 0.6000000000000001] , label 0, predict [0.0067244821544257373]\n",
      "input [0.6000000000000001, 0.3999999999999999] , label 1, predict [0.98827007750492735]\n",
      "input [0.7000000000000001, 0.7000000000000001] , label 0, predict [0.0041589747460090481]\n",
      "input [0.7000000000000001, 0.29999999999999993] , label 1, predict [0.99983116585743448]\n",
      "input [0.8, 0.8] , label 0, predict [0.0023835347349314855]\n",
      "input [0.8, 0.19999999999999996] , label 1, predict [0.99989670350602167]\n",
      "input [0.9, 0.9] , label 0, predict [0.0013812927209903121]\n",
      "input [0.9, 0.09999999999999998] , label 1, predict [0.99990525850916623]\n",
      "input [1.0, 1.0] , label 0, predict [0.00084213420992610376]\n",
      "input [1.0, 0.0] , label 1, predict [0.99990792236691806]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "inputs, labels = dataset\n",
    "pred_y = []\n",
    "for i in range(len(inputs)):\n",
    "    print(\"input {0} , label {1}, predict {2}\".format(inputs[i], labels[i], nn.predict(inputs[i])))\n",
    "    pred_y.append(nn.predict(inputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_result(inputs, labels, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
